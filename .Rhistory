libarry(ISLR)
library(ISLR)
library(caret)
data(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
x <- 1
y <- 2
x~y
plot(x~y)
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75,list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M>0.8,arr.ind=T)
library(caret)
data("faithful")
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,p=0.5,list=FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lml <- lm(eruptions ~ waiting,data=trainFaith)
summary(lml)
lines(trainFaith$waiting,lml$fitted,lwd=3)
lines(trainFaith$waiting,lml$fitted,lwd=3)
lines(trainFaith$waiting,lml$fitted,lwd=3,col="red")
coef(lml)[1]+coef(lml)[2]*80
newdata <- data.frame(waiting=80)
predict(lml,newdata)
par(mfrow=c(1,2))
plot(trainFaith$eruptions)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lml$fitted,lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lml,newdata=testFaith),lwd=3)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
Wage <- subset(age,select=-c(logwage))
Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qplot(age,wage,colour=education,data=training)
modFit <- train(wage ~ age + jobclass + education,method="lm",data=training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
plot(finMod$residuals,pch=19)
pred <- predict(modFit,testing)
qplot(wage,pred,colour=year,data=testing)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "cc40465b58430daccbeb",
secret = "a87a96e588e5881be0425e907fcd75b5f04ccbed")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "cc40465b58430daccbeb",
secret = "a87a96e588e5881be0425e907fcd75b5f04ccbed")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app("Coursera",
key = "cc40465b58430daccbeb",
secret = "a87a96e588e5881be0425e907fcd75b5f04ccbed")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth1.0_token(myapp,token="cc40465b58430daccbeb",token_secret="a87a96e588e5881be0425e907fcd75b5f04ccbed")
myapp
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
require(httpuv)
require(jsonlite)
oauth_endpoints("github")
myapp <- oauth_app("Coursera", "cc40465b58430daccbeb", secret="a87a96e588e5881be0425e907fcd75b5f04ccbed")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
output <- content(req)
list(output[[4]]$name, output[[4]]$created_at)
list(output[[4]]$name, output[[1]]$created_at)
list(output[[1]]$name, output[[1]]$created_at)
list(output[[5]]$name, output[[5]]$created_at)
output
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
packages <- c("data.table", "sqldf")
acs <- data.table(read.csv(f))
sapply(packages, require, character.only=TRUE, quietly=TRUE)
acs <- data.table(read.csv(f))
packages.install(sqldf)
installed.packages(sqldf)
packages.install(sqldf)
install.packages(sqldf)
install.packages("sqldf")
library(sqldf)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
acs <- data.table(read.csv(f))
packages <- c("data.table", "sqldf")
sapply(packages, require, character.only=TRUE, quietly=TRUE)
library(data.table)
packages <- c("data.table", "sqldf")
library(data.table)
sapply(packages, require, character.only=TRUE, quietly=TRUE)
library(sqldf)
acs <- data.table(read.csv(f))
library(RMySQL)
acs <- data.table(read.csv(f))
install.packages(data.table)
install.packages("data.table")
library(data.table)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
packages <- c("data.table", "sqldf")
sapply(packages, require, character.only=TRUE, quietly=TRUE)
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
library(sqldf)
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
packages <- c("data.table", "sqldf")
sapply(packages, require, character.only=TRUE, quietly=TRUE)
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf()
sqldf(acs)
packages <- c("data.table", "sqldf")
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
acs <- data.table(read.csv(f))
sqldf::sqldf(acs)
query2 <- sqldf("select pwgtp1 from acs")
sqldf("select pwgtp1 from acs")
library(sqldf)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
acs <- data.table(read.csv(f))
library(data.table)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
query1
query2 <- sqldf("select pwgtp1 from acs")
query3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")
query4 <- sqldf("select * from acs where AGEP < 50")
identical(query3, query4)
gold <- unique(acs$AGEP)
query1 <- sqldf("select distinct AGEP from acs")
query2 <- sqldf("select AGEP where unique from acs")
query3 <- sqldf("select unique * from acs")
query4 <- sqldf("select unique AGEP from acs")
identical(gold, query1)
identical(gold, query2)
identical(gold, query3)
identical(gold, query4)
query1 <- sqldf("select distinct AGEP from acs")
query2 <- sqldf("select AGEP where unique from acs")
query3 <- sqldf("select unique AGEP from acs")
query4 <- sqldf("select distinct pwgtp1 from acs")
identical(gold, query4)
gold <- unique(acs$AGEP)
connection <- url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode <- readLines(connection)
close(connection)
c(nchar(htmlCode[10]), nchar(htmlCode[20]), nchar(htmlCode[30]), nchar(htmlCode[100]))
require(httr)
require(XML)
htmlCode <- GET("http://biostat.jhsph.edu/~jleek/contact.html")
content <- content(htmlCode, as="text")
htmlParsed <- htmlParse(content, asText=TRUE)
xpathSApply(htmlParsed, "//title", xmlValue)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n=10)
url <- "http://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n=10)
w <- c(1, 9, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3)
colNames <- c("filler", "week", "filler", "sstNino12", "filler", "sstaNino12", "filler", "sstNino3", "filler", "sstaNino3", "filler", "sstNino34", "filler", "sstaNino34", "filler", "sstNino4", "filler", "sstaNino4")
d <- read.fwf(url, w, header=FALSE, skip=4, col.names=colNames)
d <- d[, grep("^[^filler]", names(d))]
sum(d[, 4])
library(httr)
require(httpuv)
require(jsonlite)
oauth_endpoints("github")
myapp <- oauth_app("Coursera", "cc40465b58430daccbeb", secret="a87a96e588e5881be0425e907fcd75b5f04ccbed
")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
req <- GET("http://api.github.com/users/jtleek/repos", config(token = github_token))
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
output <- content(req)
list(output[[4]]$name, output[[4]]$created_at)
req <- GET("http://api.github.com/users/jtleek/repos", config(token = github_token))
req <- GET("http://api.github.com/users/jtleek/repos", config(token = github_token))
list(output[[4]]$name, output[[4]]$created_at)
list(output[[4]]$name, output[[4]]$created_at)
library(httr)
require(httpuv)
require(jsonlite)
oauth_endpoints("github")
myapp <- oauth_app("Coursera", "cc40465b58430daccbeb", secret="a87a96e588e5881be0425e907fcd75b5f04ccbed")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
output <- content(req)
list(output[[4]]$name, output[[4]]$created_at)
output$name
output[[]]$name
output[]$name
output[:]$name
output
output[[1]]
output["repo"]
output[["repo"]]
output["repo"]
list(output[[4]]$name, output[[4]]$created_at)
list(output[[1]]$name, output[[4]]$created_at)
list(output[[2]]$name, output[[4]]$created_at)
list(output[[3]]$name, output[[4]]$created_at)
output["repo"]
output
output["datashare"]
names(output)
table(output)
head(houtput)
head(output)
head(output,10)
output[name="repo"]
output[["repo"]]
output[[repo]]
dim(output)
output[[1]]
output[[1]]$name
output[[2]]$name
output[[3]]$name
for i=1:30 {}
output[[1:30]]$name
output[[3]]$name
output[[4]]$name
output[[5]]$name
output[[6]]$name
output[[7]]$name
output[[7]]
setwd("~/workspace/Practical_Machine_Learning")
install.packages(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
head(adData)
head(adData,1)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
adData
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training)
ggplot(data=training, aes(x=Superplasticizer)) + geom_histogram() + theme_bw()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(IL, method='pca', thresh=0.9,outcome=training$diagnosis)
preProc$rotation
preProc <- preProcess(IL, method='pca', thresh=0.99,outcome=training$diagnosis)
preProc$rotation
preProc <- preProcess(IL, method='pca', thresh=0.8,outcome=training$diagnosis)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
dataframe <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(dataframe$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
training <- dataframe[inTrain, ]
testing <- dataframe[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
NONPCA <- C1$overall[1]
NONPCA
modelFit <- train(training$diagnosis ~ ., method="glm", preProcess="pca", data=training,trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
PCA <- C2$overall[1]
PCA
setwd("/home/gianfranco/workspace/Practical_Machine_Learning")
load("project-calc.R"
)
load("project-calc.R")
load("project-calc.R")
#Reading and Cleaning Data
#Read both training and testing instances. I have used a function named LOAD to load the packages that I will use later.
setwd("/home/gianfranco/workspace/Practical_Machine_Learning")
load <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
training_data <- read.csv("pml-training.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
testing_data <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
#I need to drop columns with NAs, drop highly correlated variables and drop variables with 0 (or approx to 0) variance.
str(training)
cleantraining <- training_data[, -which(names(training_data) %in% c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"))]
cleantraining = cleantraining[, colSums(is.na(cleantraining)) == 0] #this drops columns with NAs
zerovariance =nearZeroVar(cleantraining[sapply(cleantraining, is.numeric)], saveMetrics=TRUE)
cleantraining = cleantraining[, zerovariance[, 'nzv'] == 0] #to remove 0 or near to 0 variance variables
correlationmatrix <- cor(na.omit(cleantraining[sapply(cleantraining, is.numeric)]))
dim(correlationmatrix)
correlationmatrixdegreesoffreedom <- expand.grid(row = 1:52, col = 1:52)
correlationmatrixdegreesoffreedom$correlation <- as.vector(correlationmatrix) #this returns the correlation matrix in matrix format
removehighcorrelation <- findCorrelation(correlationmatrix, cutoff = .7, verbose = TRUE)
cleantraining <- cleantraining[, -removehighcorrelation] #this removes highly correlated variables (in psychometric theory .7+ correlation is a high correlation)
for(i in c(8:ncol(cleantraining)-1)) {cleantraining[,i] = as.numeric(as.character(cleantraining[,i]))}
for(i in c(8:ncol(testing_data)-1)) {testing_data[,i] = as.numeric(as.character(testing_data[,i]))} #Some columns were blank, hence are dropped. I will use a set that only includes complete columns. I also remove user name, timestamps and windows to have a light data set.
featureset <- colnames(cleantraining[colSums(is.na(cleantraining)) == 0])[-(1:7)]
modeldata <- cleantraining[featureset]
featureset #now we have the model data built from our feature set.
##Cross-Validation
#I need to split the sample in two samples. This is to divide training and testing for cross-validation.
idx <- createDataPartition(modeldata$classe, p=0.6, list=FALSE )
training <- modeldata[idx,]
testing <- modeldata[-idx,]
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
m
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
model
predict <- predict(model, testing)
confusionMatrix(testing$classe, predict)
accuracy <- postResample(predict, testing$classe)
accuracy
result <- predict(model, training[, -length(names(training))])
result
treeModel <- rpart(classe ~ ., data=cleantraining, method="class")
prp(treeModel)
##############################
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
testing_data <- testing_data[featureset[featureset!='classe']]
answers <- predict(model, newdata=testing_data)
answers
pml_write_files(answers)
setwd("/home/gianfranco/workspace/Practical_Machine_Learning")
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
setwd("/home/gianfranco/workspace/Practical_Machine_Learning")
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
load("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
setwd("/home/gianfranco/workspace/Practical_Machine_Learning")
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
load <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
load <- function(package){
new.package <- package[!(package %in% installed.packages()[, "Package"])]
if (length(new.package))
install.packages(new.package, dependencies = TRUE)
sapply(package, require, character.only = TRUE)
}
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"
if (!file.exists(trainFile)) {
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile=trainFile, method="curl")
}
if (!file.exists(trainFile)) {
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile=testFile, method="curl",quiet=TRUE)
}
training <- read.csv(trainFile)
testing <- read.csv(testFile)
dim(training)
dim(testing)
summary(training$classe)
sum(complete.cases(training))
trainingCleaned <- training[, colSums(is.na(training)) == 0]
dim(trainingCleaned)
nzv <- nearZeroVar(trainingCleaned, saveMetrics=TRUE)
load(caret)
libary(caret)
library(caret)
nzv <- nearZeroVar(trainingCleaned, saveMetrics=TRUE)
trainingCleaned <- trainingCleaned[, nzv[,"nzv"] == FALSE]
dim(trainingCleaned)
trainingCleaned <- trainingCleaned[, -grep("timestamp|user_name|new_window|num_window|X", names(trainingCleaned))]
dim(trainingCleaned)
set.seed(1234)
inTrain <- createDataPartition(trainingCleaned$classe, p=0.7, list=FALSE)
trainingData <- trainingCleaned[inTrain,]
validationData <- trainingCleaned[-inTrain, ]
modelRF <- train(classe ~ .,
data=trainingData, method="rf",
trControl=trainControl(method="cv", number=5),
prox=TRUE,
allowParallel=TRUE,
ntree=250)
modelRF
model <- train(classe ~ .,
data=trainingData, method="rf",
trControl=trainControl(method="cv", number=5),
prox=TRUE,
allowParallel=TRUE,
ntree=250)
library(caret)
model <- train(classe ~ .,
data=trainingData, method="rf",
trControl=trainControl(method="cv", number=5),
prox=TRUE,
allowParallel=TRUE,
ntree=250)
numCores <- detectCores()
registerDoMC(cores = numCores - 1)
install.package("doMC")
install.packages("doMC")
install.packages(doMC)
install.packages("doMC")
install.packages("doMC")
